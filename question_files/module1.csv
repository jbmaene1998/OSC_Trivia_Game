question,answer,null
"What does it truly mean if people say: ""computers work on 0's and 1's""?",Computers work on 0's and 1's because they are the only two values that can be represented by a single bit. This is the basic unit of information that a computer can process.,
The Difference Between a Framework and a Library,A framework provides the blueprint and the basic framework; it also shows what is still needed from the programmer in terms of customisation. The framework provides the flow of a software application and tells the developer what it needs and calls the code provided by the developer as required. If a library is used; the application calls the code from the library. A framework inverts the control of the program. It tells the developer what they need. A library doesn’t. The programmer calls the library where and when they need it,
What is the difference between a compiler and an interpreter?,A compiler is a program that translates source code into machine code. An interpreter is a program that translates source code into machine code line by line,
What type of programming languages exist? Why are some called high- and others low-level programming languages?,Java; Python; PHP; C; C# and C++ are examples of programming languages. High-level languages are designed to be easy for humans to write and understand; but they are slower to execute. Conversely; low-level languages are harder for humans to write and understand; but they are faster to execute. Python and C# are examples of high-level languages that are widely used in education and in the workplace because they are user-oriented and make it easy for programmers to convert algorithms into program code. Low-level languages; such as assembly language and machine code; are machine-oriented and are expressed in terms of the machine operations that must be performed to carry out a task. This makes writing programs more difficult as the algorithm must be specified in terms of the capabilities and specifications of the processor. Low-level languages are named for the processor or processor family they are designed for.,
What is the difference between assembly code and machine code? What is disassembly in this context?,Machine code is a computer program written in machine language instructions that can be executed directly by a computer's central processing unit (CPU). Assembly language is a low-level programming language that is more advanced than machine code but less advanced than high-level languages. It uses acronyms; symbols; and numbers instead of binary digits (1s and 0s) and it is more human-readable than machine code. The difference between machine code and assembly language is that machine code is only understandable by computers whereas assembly language can be understood; used; and applied by humans. Disassembly is the process of converting machine code into assembly language; it is the reverse process of assembly and it is used to understand and analyze the underlying machine code of a program or to recreate the source code.,
What is a PCB (Printed Circuit Board) (in a hardware context) and how are electronics and logical gates related?,A printed circuit board (PCB) mechanically supports and electrically connects electronic components or electrical components using conductive tracks; pads and other features etched from one or more sheet layers of copper laminated onto and/or between sheet layers of a non-conductive substrate. Components are generally soldered onto the PCB to both electrically connect and mechanically fasten them to it.,
Give 5 names of different Operating Systems (flavors),The most popular operating systems are Windows; Mac OS; Linux; Android; and iOS,
How old are they?,Windows 1.0 was released in 1985. Mac OS X was released in 2001. Linux was released in 1991. Android was released in 2008. iOS was released in 2007.,
What is a Linux distribution?,A Linux distribution (or distro for short) is an operating system made from a software collection. It is based on the Linux kernel and; usually; consists of GNU software and a desktop environment. The defining component of a Linux distribution is the package management system. Distributions provide sets of software built from the source code of free and open-source software (FOSS) projects. They are typically developed by a group of volunteers; and are often free of charge.,
"Give a definition for ""Operating System"" (in a classic sense and as defined in this course)",An operating system (OS) is system software that manages computer hardware and software resources and provides common services for computer programs. All computer programs; excluding firmware; require an operating system to function.,
Do a microwave; a fridge or a classic Game Boy have operating systems?,No; they do not.,
Explain the difference between virtualisation; emulation and simulation?,Virtualization is the creation of a virtual (rather than actual) version of something; such as an operating system; a server; a storage device or network resources. Emulation is the imitation of the behaviour of one system (called the emulated system) by another (called the host system). Simulation is the imitation of the operation of a real-world process or system over time.,
What is a mainframe?,The mainframe is a large; expensive computer that is used by large organizations for critical applications; bulk data processing such as census; industry and consumer statistics; enterprise resource planning; and transaction processing. Mainframes are used in large companies and government agencies. They are also used in smaller companies that have outgrown minicomputers.,
What is/are the primary goal(s) of an Operating System?,The primary goal of an operating system is to provide an environment in which a user can execute programs in a convenient and efficient manner. The operating system is a set of programs that manages the computer's resources and provides common services for computer programs.,
"Why is there a new way of looking at ""Operating Systems"" (in a cloud / micro service / more modern sense)?",The new way of looking at operating systems is because of the cloud. The cloud is a network of remote servers hosted on the Internet to store; manage; and process data; rather than a local server or a personal computer. Cloud computing is a type of Internet-based computing that provides shared computer processing resources and data to computers and other devices on demand. It is a model for enabling ubiquitous; convenient; on-demand network access to a shared pool of configurable computing resources (e.g.; networks; servers; storage; applications; and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.,
Explain the difference between User space and Kernel space.,Kernel space is strictly reserved for running a privileged operating system kernel; kernel extensions; and most device drivers. In contrast; user space is the memory area where application software and some drivers execute.,
What are system calls?,A system call is a request by a program for a service from the kernel of the operating system. System calls are the primary method by which an application program requests a service from the operating system.,
What are drivers?,A driver is a computer program that operates or controls a particular type of device that is attached to a computer. A driver provides a software interface that allows hardware devices to be used by other computer programs.,
What made Pascal so special?,Pascal’s Calculator is a mechanical calculator invented by Blaise Pascal in 1642. Pascal was led to develop a calculator by the laborious arithmetical calculations required by his father's work as the supervisor of taxes in Rouen. He designed the machine to add and subtract two numbers directly and to perform multiplication and division through repeated addition or subtraction.Pascal was a computer language developed by Niklaus Wirth in 1970. It was designed to be easy to learn and use; and to encourage good programming practices using structured programming and data structuring. Pascal was named in honor of the French mathematician; philosopher and physicist Blaise Pascal.,
Why are people like Charles Babbage and Ada Lovelace so critical in the evolution of (modern) computers?,"Charles Babbage was an English mathematician and mechanical engineer who originated the concept of a programmable computer. He invented the first mechanical computer in the early 19th century and considered his Analytical Engine a design for a general-purpose computer. Ada Lovelace was an English mathematician and writer chiefly known for her work on Charles Babbage's early mechanical general-purpose computer; the Analytical Engine. She was the first to recognise that the machine had applications beyond pure calculation; and published the first algorithm intended to be carried out by such a machine. As a result; she is often regarded as the first to recognise the full potential of a ""computing machine"" and the first computer programmer.",
Was there ever such a thing as a non-electronic computer?,Yes; there have been computers that were not electronic. These were called mechanical computers or analog computers. These types of computers were used in the early 20th century and were typically used for tasks such as navigation and artillery calculations. They used gears; levers; and other mechanical components to perform calculations and make decisions based on physical inputs. They were gradually replaced by electronic computers in the second half of the 20th century because of their greater speed; accuracy; and flexibility.,
How did Alan Turing win the war?,Alan Turing; a British mathematician and computer scientist; played a crucial role in cracking the German Enigma code during World War II. He was part of a team at Bletchley Park; the British government's codebreaking center; that worked to decipher the encoded messages sent by the German military. Using a combination of clever mathematical techniques and early computing machinery; such as the Bombe machine; Turing and his colleagues were able to break the Enigma code and provide valuable intelligence to the Allies. This work is considered to have significantly shortened the war and saved many lives.,
Which functions does a punch card have for a computer?,A punch card; also known as a Hollerith card; is a piece of stiff paper or cardboard that contains holes punched in specific patterns. These patterns represent data or instructions for a computer.The use of punch cards for computers dates back to the early days of computing; where they were used to input data and instructions into the computer and store programs. The cards were read by a card reader; which would interpret the hole patterns and convert them into machine-readable form. The program or data on the punch card could then be processed by the computer.Punch cards were primarily used in the input/output of data and the preparation of program instructions for computers. They had many advantages such as the ability to store large amount of data and be easily transported. They were also more reliable than paper tape. Punch cards were extensively used to input data for census; scientific research and many other applications.It's worth noting that with the development of magnetic tape and disk storage as well as the keyboard; mouse and monitor as a standard input/output device; Punch card usage has become obsolete; as it is replaced by other forms of data storage and input.,
What do we mean when we talk about the Von Neumann bottleneck?,The Von Neumann bottleneck refers to the fundamental limitation on a computer's processing speed imposed by the need to access data stored in memory. The concept is named after the mathematician and computer scientist John von Neumann; who first proposed the basic architecture of modern computers.In this architecture; there is a clear separation between the memory; where data is stored; and the processor; which performs calculations on that data.In order for the processor to work on a piece of data; it needs to first retrieve it from memory. The speed at which data can be retrieved from memory is often slower than the speed at which the processor can work on it; creating a bottleneck in the system. Because of this bottleneck; the processor has to spend a significant amount of time waiting for data to be retrieved from memory; and this limits the overall performance of the computer. Improving memory technology; data transfer between memory and processor and improved architectural design of computers which optimized the data processing pipeline and also reduced the data access time; is the approaches taken to mitigate or alleviate this bottleneck. It's worth noting that; Von Neumann bottleneck was not a problem in early days of computing when memory was small and slow; however as computer systems has become more powerful; the gap between the speed of processors and memory has increased and it become a significant performance issue.,
How does Moore's Law impact your cost of digital living?,Moore's Law is an observation made by Gordon Moore; co-founder of Intel; in 1965. He noted that the number of transistors on a microchip (a fundamental component of modern computers) was doubling approximately every 18-24 months; and that this would lead to a corresponding increase in the processing power and performance of computers. The impact of Moore's Law on the cost of digital living is that as computers become more powerful; the cost of using technology decreases. This is because more powerful computers can perform the same tasks more efficiently; which allows for new and more advanced applications to be developed. For example; as a result of Moore's Law; personal computers have become smaller; more powerful; and less expensive over time; making it possible for more people to afford to own one. Similarly; mobile devices like smartphones and tablets have become more powerful and less expensive; making it easier and more affordable for people to stay connected to the internet and access digital services. Additionally; Moore's Law has also enabled industries such as education; healthcare; transportation and many more to use technology in a more efficient way; which results in cost savings and more accessible services. It's worth noting that; Moore's Law has been slowing down due to the physical limits of scaling down the semiconductor technology used to create transistors and the cost of producing these smaller chips have also increased; which is causing the cost of computer systems to increase.,
Why was Unix made at all?,Unix was developed in the late 1960s and early 1970s at Bell Labs; which was then a research and development arm of AT&T. The primary goal of the Unix project was to create a more powerful and flexible operating system for the company's computers. Unix was designed to be highly portable and easily adaptable; so that it could be used on a wide variety of computers; from mainframes to minicomputers. It was also designed to be able to support many different users at the same time; allowing for the development of more sophisticated and powerful applications. In summary; Unix was created to be an efficient; powerful; and adaptable operating system that could be used across different types of computers; support multiple users and enable the development of more sophisticated applications.,
Why was Dennis Ritchie a crucial element in the evolution of the computer?,In summary; Dennis Ritchie played a crucial role in the evolution of computers through his contributions to the development of the Unix operating system and the C programming language. These two technologies have had a profound impact on the way that computers are used and programmed today; and his ideas and practices continue to influence the field of computer science.,
Who was Ken Thompson?,Ken Thompson is a computer scientist and engineer; known for his contributions to the development of the Unix operating system and the C programming language. Along with Dennis Ritchie; he was one of the main developers of Unix at Bell Labs in the 1970s.,
What was the role of the evolution of B & C in the creation of Unix and subsequent Operating Systems?,The evolution of the B and C programming languages played a crucial role in the creation of the Unix operating system. The B language was the initial language used by the creators of Unix; but its limitations led to the development of C; which offered more efficient and powerful way to program operating systems and provided Unix with a more flexible and efficient foundation for future development.,
What are the core principles of the Unix Philosophy?,"Make each program do one thing well. To do a new job; build afresh rather than complicate old programs by adding new ""features"". Expect the output of every program to become the input to another; as yet unknown; program. Don't clutter output with extraneous information. Avoid stringently columnar or binary input formats. Don't insist on interactive input. Design and build software; even operating systems; to be tried early; ideally within weeks. Don't hesitate to throw away the clumsy parts and rebuild them.Use tools in preference to unskilled help to lighten a programming task; even if you have to detour to build the tools and expect to throw some of them out after you've finished using them.",
Why is worse (sometimes) better in modern operating systems?,"""worse is better"" is a concept that argues that in some cases; a less powerful and feature-rich operating system or software program can be more successful in the long run than a more complex and feature-rich alternative; because it is simpler; more efficient and more reliable; which makes it easier to use; understand and maintain.",
What is POSIX?,POSIX (Portable Operating System Interface) is a set of standards for operating systems. It is an IEEE standard that defines a common interface for operating systems; including the interface between an operating system and application programs. The goal of POSIX is to ensure that applications written for one POSIX-compliant operating system can be run on another POSIX-compliant operating system without modification.,
What is UTF 32-16-8?,UTF-32; UTF-16 and UTF-8 are all Unicode transformation formats (UTF); they're different ways of encoding the same set of Unicode characters. UTF-32 uses fixed 4 bytes per character; UTF-16 uses fixed 2 bytes (or 4 bytes if needed) per character; while UTF-8 uses variable number of bytes per character. UTF-8 is considered the most efficient and widely used representation among them; as it's the most space-efficient when representing text that primarily uses characters from the ASCII character set; and it's backward-compatible with ASCII.,
What is a distro?,A distro is a version of a Linux-based operating system that is built from the Linux kernel; along with a collection of software applications and tools. Each distribution is maintained and developed by a community of developers and has its own set of features; philosophies; and target users.,
How do you find out wich distros are most used and why would you want to know?,Survey-based statistics; Web analytics; Market share; Package management statistics. Knowing which distributions are most used can be helpful for several reasons: Software compatibility; Hardware support; Community support.,
